################### Основы статистики
#
##Базовые понятия
#
#Генеральная совокупность - множество объектов, о которых мы хотели бы сделать выводы
#в рамках нашего исследования
#
#Выборка - часть генеральной совокупности
#
#Репрезентативная выборка - случайным образом сформированная выборка, которая отражает
#свойства генеральной совокупности. Выборка является репрезентативной в зависимости от
#способа её формирования
#
#Способы формирования выборки:
#1)Простая случайная выборка - случайным образом выбраны несколько представителей из ген.сов
#2)Стратифицированная выборка - сначала генеральная совокупность разбивается на несколько
#страт, и из каждой страты делаем случайную выборку
#3)Групповая выборка - совокупность делится на несколько групп(кластеров), похожих друг
#на друга(например несколько географисеских районов одного города) и делается 
#случайная выборка из каждой группы
#
##Типы переменных:
#1)Количественные:
#a)Непрерывные - переменные, которые могут принимать любое значение (от 100 до 200)
#б)Дискретные - переменные, которые принимают определенное значение (1,2,3,4)
#2)Качественные(Номинативные) - переменные, которые делят наши данные на группы
#3)Ранговые переменные - переменные, которые позволяют сравнивать разные наблюдения
#
#
##############Типы описательной статистики:
#
#1)Меры центральной тенденции - отвечают на вопрос насколько высокие значения принимает
#наша переменная
#
#2)Меры изменчивости - отвечают на вопросы изменчивости нашей переменной
#
##Меры центральной тенденции
#
#1)Мода(Mode) распределения - значение измеряемого признака, которое встречается
#наиболее часто (таких наблюдений может быть несколько)
#Свойство унимодальности - это свойство, когда у распределения одна мода
#
#2)Медиана(median) - значение, которое делит упорядоченное множество данных на две части
#
#3)Среднее значение(mean) - среднее арифметическое, сумма всех значений деленая 
#на их количество
#
#
#Свойства стеднего значения(M):
#1)Если к каждому члену выборки прибавить константу, то среднее увеличится на константу
#M(x+c) = Mx + C
#2)Если каждый член выборки умножить на константу, то среднее значение тоже умножается
#M(x*c) = Mx*c
#3)Сумма отклонений каждого члена выборки от среднего по выборке равна нулю
#E(xi-Mi) = 0
#
###Меры изменчивости
#
#1)Размах (Range) - разность между максимальным и минимальным значением нашего признака
#R = Xmax - Xmin
#2)Дисперсия(variance)- среднее арифметическое квадратовотклонений каждого наблюдения 
#от среднего значения по выборке
#D = E(xi-M)^2/n-1 (n для генеральной совокупности, n-1 для выборки)
#3)Среднее квадратическое отклонение(сигма для генеральной совокупности, Sd для выборки) 
#это квадратный корень из дисперсии (D)^0.5
#
#Свойства дисперсии:
#1)Если к каждому члену совокупности прибавить константу, то дисперсия 
#и стандартное отклонение не изменятся
#D(x+c) = Dx
#2)Если каждое значение выборки умножить на константу, то дисперсия и стандартное отклонение
#умножатся на это число
#D(x*c) = Dx*c^2
#Sd(x*c) = Sdx*c
#
#Квантили - такие значения признака, которые делят упорядоченные данные на несколько частей
#Квартиль - делит на 4 части нашу выборку
#Используются например на boxplot 
#
######Нормальное распределение
#Свойства нормального распределения:
#1)Унимодально - имеет одну моду
#2)Симметричное - симметрично относительно центральной оси
#3)Отклонения от среднего значения подчиняются определенному вероятностному закону
#
#Z-стандартизация - преобразование наших данных в стандартную Z-шкалу, где 
#среднее значение равно нулю(Mz = 0), а дисперсия равна единице (Dz = 1)
#Zi = (xi -xср)/Sd
#
#В нормальном распределении
#Мx +- Sd - примерно 68% всех наблюдений
#Mx +- 2Sd - примерно 95% всех наблюдений
#Mx +- 3Sd - примерно 100% всех наблюдений
#
#######Центральная предельная теорема
#
#Стандартная ошибка среднего(Se) - Стандартное отклонение среднего выборки от 
#среднего в генереальной совокупности
#
#Se = Sd/sqrt(n), где n - это число наблюдений в выборке
#
##Доверительный интервал это такой интервал относительно которого мы можем быть уверены
#в том, что он включает в себя определенные параметры (например Mср+- 1,96Se дов. интервал)
#
#Идея статистического вывода:
#H0 - нулевая гипотеза, H1 - проверяемая гипотеза
#
#Ошибки статистического вывода:
#1)Ошибка 1го рода - отклонение нулевой гипотезы, которая на самом деле верна
#2)Ошибка 2го рода - отклонение проверяемой гипотезы, которая на самом деле верна
#
##Т-распределение (распределение Стьюдента)(выше нормального)
#https://gallery.shinyapps.io/CLT_mean/ ссылка на приложение
#https://gallery.shinyapps.io/dist_calc/ ссылка на приложение
#При количество исследований меньше N<30 нарушается утверждение о том, что все выборочные
#средние будут распределяться вокруг среднего генеральной совокупности в соответствии
#с нормальным законом
#
#Форма Т-распределения зависит от степени свободы (sd = n-1), чем выше sd тем ближе 
#Т-распределение к нормальном
#
#z - единицы стандартной ошибки среднего 
#z = (x(ср)-m)/Se (t = (x(ср)-m)/Se) используется с T-распределением)
#где m - среднее ген. совокупности
#x(ср) - среднее выборки
#Se - стандартная ошибка среднего (Se = Sd/sqrt(n))
#
#df - количество элементов, которые могут варьироваться при вычислении например
#если есть 9 значений из 10, и среднее, тогда 10ое нам не нужно значит df = 9
#
###Парный T-тест (критерий Т-Стьюдента)
#
#Критерий, который позволяет сравнивать две выборки между собой
#
#Для использования Т-теста необходимы 2 условия
#1)Гомогенность дисперсий (можно оценивать по кретерию Левена, или Фишера)
#2)Нормальность распределения(тест Шапиро-Уилка, тест Колмогорова Смирнова, 
#а так же определяется на графике QQ-plot например)
#тест Шапиро-Уилка тестирует нулевую гипотезу о том, что наше распределение значимо НЕ 
#отличается от нормального, если p-уровень значимости <0.05 тогда распределение не нормально
#Например:
#H0: m1-m2 = 0 (средние генеральных совокупностей двух выборок равно)
#H1: m1-m2 !=0
#
#Se = ((sd1^2)/n1 + (sd2^2)/n2)^0.5
#
#t = ((x1ср-x2ср) - (m1-m2)(0))/Se
#
#t = ((x1ср-x2ср) - (m1-m2)(0))/((sd1^2)/n1 + (sd2^2)/n2)^0.5
#
#по t значению определяется p-уровень значимости
#
#Выброс - экстремально высокое или экстремально низкое значение 
#отличие на более чем полтора межквартильных размаха
#
#U-Тест (У-критерий Манна-Уитни)
#У-критерий Манна-Уитни применяется в случае сильного отличия распределения нашей выборки
#от нормального
#У-критерий Манна-Уитни переводит все наши наблюдения в ранговую шкалу, таким образом
#он менее чувствителен к выбросам
#
##Однофакторный дисперсионный анализ (ANOVA в R)
#
#https://gallery.shinyapps.io/CLT_mean/ ссылка на приложение
#https://gallery.shinyapps.io/dist_calc/ ссылка на приложение
#
##Для использования дисперсионного анализа необходимы 2 условия:
#1)Гомогенность дисперсий (можно оценивать по кретерию Левена, или Фишера)
#2)Нормальность распределения(тест Шапиро-Уилка, тест Колмогорова Смирнова, 
#а так же определяется на графике QQ-plot например)
#
#Применяется для сравнения нескольких групп наблюдений
#Очень часто в экспериментах и исследованиях возникает необходимость сравнить 
#несколько групп между собой. В таком случае мы можем применять однофакторный
#дисперсионный анализ.  
#
#Та переменная, которая будет разделять наших испытуемых или наблюдения на группы 
#(номинативная переменная с нескольким градациями) называется независимой переменной. 
#А та количественная переменная, по степени выраженности которой мы сравниваем группы, 
#называется зависимая переменная. 
#
#ПРимер для 3 групп
#H0 m1 = m2 = m3
#H1 m1 != m2 or m2 != m3
#
#Сначала считается общее среднее объединяя все три группы в одну Xcр
#
#Далее считается SST - sum squares total для Xcp
#
#SST = E(x-Xср)^2
#
#df = N-1
#
#SSB - sum squares between groups (сумма квадратов отклонения межгруппового)
#SSB = En*(xср-m)^2 
#df(SSB) = m-1 где m - число групп
#SSW - sum squares within groups (сумма квадратов внутри группы)
#SSW = E(x-xср)^2 
#df(ssw) = N-m, где N - число наблюдений, m - число групп
#
#SST = SSB + SSW = E(x-Xср)^2 (формула выводится отдельно)
#
#F - значение - это основной статистический показатель дисперсионного анализа
#F - значение отмечается на F - распределении и по нему высчитывается p - уровень значимости
#https://gallery.shinyapps.io/CLT_mean/ ссылка на приложение
#https://gallery.shinyapps.io/dist_calc/ ссылка на приложение
#
#F = (SSB/df(ssB))/(SSW/df(SSW))
#
#SSB - при верности нулевой гипотезы о равном среднем должно стремиться к нулю
#так как среднее между группам должны минимально отличаться
#
#чем меньше SSB тем меньше F- значение и тем
#
#Чем больше F-значение тем меньше p-уровень значимости,значит средние в гс от выборок разные
#
####Множественные сравнения в дисперсионном анализе (ANOVA в R)
#
#Для поправок на множественные сравнения можно использовать:1)Поправку Бонферрони
#2)Критерий Тьюки(Tukey HSD), 3)False discovery rate и много разных поправок, которые
#можно посмотреть в пакете, когда будем делать дисперсионный анализ
#
#Проблема множественного сравнения выборок:
#Если многократно увеличивать число групп, то шанс получить статистически значимые 
#различия стремится к 100%
#
#Для того, чтобы корректировать p-значение на множественные сравнения существует
#поправка Бонферрони согласно которой pn = p0/(n*(n-1)/2), где p0 - это 
#p-уровень значимости (обычно 0.05)
#
#Поправка Бонферрони критикуется за свою грубость(консервативность)
#
#Поправка Тьюки(Tukey HSD)
#
#Вычисление критерия Тьюки:
#q = (x(ср)B - x(ср)A)/SE, SE = sqrt(MSWW/n) где MSw - внутригрупповая дисперсия.
#Приведённая формула верна для случаев, 
#когда все сравниваемые группы содержат одинаковое число наблюдений n. 
#Если же сравниваемые группы неодинаковы по размеру, то
#SE=sqrt(MSw*(1/nA+1/nB))
#
##Многофакторный дисперсионный анализ (ANOVA в R)
#Позволяет оценить влияние нескольких факторов и их взаимодействия на нашу переменную
#
#На примере двух факторов:
#SST(sum squares total) = SSW + SSB(a)+ SSB(b) + SSB(a)*SSB(b)  
#
##Корреляция Пирсона(так же существуют коэффициенты корреляции Спирмена и Тау Кендалла
#для нелинейных взаимосвязей)
#
#Коэффициент корреляции показывает силу и взаимосвязь двух переменных
#
#Условия применения коэффициента корреляции:
#1)Характер взаимосвязи переменных должен быть линейным и монотонным
#можно посмотреть например на диаграмме рассеивания dotplot
#2)Характеристики распределения должны быть нормальными, так как мы опираемся
#на среднее значение, которое зависит от выбросов
#
#Положительная корреляция двух переменных - это когда с увеличением одной переменной, 
#вторая возрастает
#
#Отрицательная корреляция двух переменных - это когда с увеличением одной переменной, 
#вторая убывает
#
#Вывод формулы коэффициента корреляции: 
#
#Cov = E(xi-xср)*(yi-yср)/(N-1) (ковариация) 
#
#Чтобы привести Cov к значение [-1; 1] (коэффициент корреляции rxy):
#
#rxy = cov/6x*6y, где 6x и 6y - это сигмы(стандартные отклонения выборок)
#
#https://rpsychologist.com/d3/correlation/ Визуализация коэффициента корреляции
#
#Квадрат коэффициента корреляции R^2 называется коэффициентом детерминации показывает 
#в какой степени дисперсия одной переменной обусловлена влиянием другой переменной
#
#Обчыно проверяется H0 о том, что rxy = 0 и H1 о том, что rxy !=0  
#расчет через cor.test(x, y)
#
#Коэффициенты корреляции Спирмена
#
#Коэффициенты корреляции Спирмена и Тау Кендалла применяются например
#при нелинейной зависимости переменных (работают примерно одинаково)
#
#rs = 1-(6*Edi^2/N*(N^2-1)) коэффициент корреляции Спирмена
#
#Коэффициенты корреляции Спирмена ранжирует все значения нашей переменной
#
#Ошибка корреляции: значение корреляции не гарантирует причинно-следственную зависимость
#но может означать эту зависимость
#
##Простая линейная регрессия(Одномерный регрессионный анализ )
#
#Условия применения регрессионного анализа:
#1)Линейная зависимость между переменными
#2)Нормальность распределения
#3)Гомоскедастичность - постоянность изменчивости остатков на всех уровнях независимой
#переменной
#
#Линия регрессии строится на графике, где по оси x - независимая переменная, 
#а по оси y зависимая
#
##y=b0+b1x, где коэффициенты b0 - intercept b1 - slope
#
#Метод построения регрессионной прямой - метод наименьших квадратов
#
#Метод наименьших квадратов - это метод нахождения оптимальных параметров 
#регрессионной прямой(b0,b1) таких, что сумма квадратов остатков(отклонений)
#значений переменных от этой прямой будет минимальным
#
#Качество регрессионной прямой можно оценить по сумме квадратов остатков
#
#b1 = Sdy/Sdx*rxy - наклон переменной (slope)
#b0 = yср - b1*xср - смещение по y (intercept)
#
#Н0 обычно предполагает, что b1 = 0
#H1 обычно предполагает, что b1 !=0
#
#R^2 = 1- SSres/SStotal - это доля дисперсии зависимой переменной объясняемая 
#нашей регрессионной моделью
#где SSres(Sum Squares) - это сумма квадратов расстояний от значений переменной до 
#линии регрессии 
#SStotal(Sum Squares) - это сумма квадравтов расстояний от значений переменной 
#до горизонтали средней
#
#t = b1/Se (t-распределение нашего b1 вокруг 0 (согласно нулевой гипотезе)) 
#df = N-2
#
#Одномерный регрессионный анализ позволяет понять:
#
#1)Какова линия регрессии
#2)Построить уравнение регресси
#3)Проверить гипотезу о статистической значимости
#4)Определить процент изменчивости зависимой переменной от независимой
#
##Предсказание значения
#Регрессионную прямую иногда называют линией тренда, согласно ней можно предсказывать
#значения зависимой переменной относительно независимой
#
#Для того, чтобы получить предсказанное значение необходимо подставить в уравнение
#линейной регрессии значение независимой переменной
#
#Предсказания имеют ограничения, например из-за отрицательных значений линии тренда
#или из-за нелинейности зависимости между переменными на некотром этапе
#
##Множественная регрессия
#Множественная регрессия позволяет исследовать влияние нескольких независимых переменных
#на одну зависимую
#
#Условия применения:
#1)Линейность зависимости
#2)Нормальность распределения остатков
#3)Гомоскедастичность - равномерность изменения данных
#4)Проверка на мультиколинеарность - связи между независимыми переменными(сонаправленность)
#5)Нормальность распределения
#
#y= b0 +b1*x1+b2*x2... +bn*xn (это не прямая, а плоскость или более сложная фигура)
#
#В данном случае R^2 необходимо рассчитывать с поправкой на множественные предикторы
#
#Регрессионный анализ дает данные по каждой переменной, коэффициенты  в уравнении
#дают информативность о влиянии переменных на зависимую переменную
#
#Наилучшая модель выбирается по значению R^2, удалением из неё различных коэффициентов,
#(b1,b2...bn), которые не улучшают модель, а ухудшают её.
#
##Логистическая регрессия
#ПОказывает зависимость номинативной(!!!) переменной от независимых
#Метод, который позволяет определить связь между зависимой переменной с двумя градациями
#то есть номенативной переменной(например поступление в вуз(поступил или не поступил))
#и различными независимыми переменными
#
#расчет влияния несколькоих независимых переменных на номинативную
#
#pi = b0 + b1*x1 + b2*x2 ... +bn*xn, где pi - это вероятность наступления события 
#
#так как pi лежит в отрезке [0;1], необходимо привести уравнение в соответствующий вид
#
#pi = exp(b0 + b1*x1 + b2*x2 ... +bn*xn)/(1+exp(b0 + b1*x1 + b2*x2 ... +bn*xn)
#
#Odds - отношение вероятности положительного исхода к отрицательному.
#например если успех - 0.2, то неуспех - 1 -0.2, значит odds = 1/1-0.2 = 0.25
#Далее логорифмируем log(odds) он принадлежит промежутку (-бесконечность; +бесконечность)
#
#log(odds) = b0+b1*x1 =>pi = exp(b0 + b1*x1)/(1+exp(b0 + b1*x1)
#
##кластерный анализ
#кластеризует данные на несколько кластеров
#
###################Виды анализа данных:
#
#1)Дисперсионный анализ(ANOVA)
#F -значение
#2)Критерий T-Стьюдента(Т-тест)
#t-значение
#3)Регрессионный анализ
#y=b0+b1x, t = b1/S
#4)корреляция
#rxy = cov/6x*6y
#5)Логистическая регрессия
#pi = b0 + b1x1+b2x2 ... +bnxn, где pi - это вероятность наступления события
#6)кластерный анализ
#кластеризация данных
#7)Критерий Хи^2 
#сравнение фактических данных с ожидаемыми
#Хи^2 = (oр-Eр)^2/Ep+(Оо-Eo)^2/Eo
#8)Точный критерий фишера
#p(x=a|x+y = a+c) = C(a/a+b)*C(c/c+d) / C(a+c/n) 
