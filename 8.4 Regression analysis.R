##########Регрессионный анализ
#
#Грубо говоря все методы анализа данных можно заменить регрессионным анализом, методы
#регрессии позволяют решать все задачи.
#
#Если зависимая переменная - количественная, тогда можно использовать линейную регрессию
#Если зависимая переменная - фактор, тогда можно использовать логистическую регрессию
#
#Ограничения на использование регрессии:
#1)Линейность связь зависимой переменной и независимой переменной
#2)Независимость наблюдений
#3)Независимость предикторов
#4)Нормальность распределения остатков
#5)Гомоскедастичность - равномерность распределения остатков
#6)Отстствие автокорреляции остатков
#(альтернативные требования)
#1)Линейность зависимости
#2)Нормальность распределения остатков
#3)Гомоскедастичность - равномерность изменения данных
#4)Проверка на мультиколинеарность - связи между независимыми переменными(сонаправленность)
#5)Нормальность распределения
#
#Наилучшими коэфициентами b0,b1 - являются те, при которых квадраты ошибок минимальны.
#как только нарушаются требования к применению линейной регрессии - метод наименьших 
#квадратов может давать неверные результаты(даже противоположные действительности)
#
####1)Линейность взаимосвязи:
#Если взаимосвязь нелинейная, то Линейная регрессия подстраивается под нелинейную 
#зависимость, чтобы минимизировать квадраты остатков, что приводит к неверным предсказаниям 
#и неверной интерпретации коэффициентов
#
#Проверить взаимосвязь можно на scatterplot
#
#Чтобы добиться более линейной взаимосвязи переменных можно трансформировать наши переменные
#
#Способы трансформации:
#1)Трансформация Тьюки(возведение в степень).
#Основа идеи - трансформация независимой переменной, чтобы ликвидировать нелинейность связи
#
#Метод трансформации - возведение независимой переменной в степень(по таблице)
#
#X = {-(x)^(n) if n<0; 
#       log(x) if n=0; 
#        x^n if n>0}
#
library(ggplot2)
qplot(x=hp, y = mpg, data = mtcars)
qplot(x=hp^0.5, y = mpg, data = mtcars)           #возведение в степень 0.5
qplot(x=hp^-0.5, y = mpg, data = mtcars)          #возведение в степень -0.5 без знака минус
qplot(x=-hp^0.5, y = mpg, data = mtcars)          #возведение в степень -0.5 со знаком минус

#rcompanion::transformTukey подгоняет значение лямба, чтобы распределение исходной 
#переменной стало более похожим на нормальное. В целом, задача решается элементарным 
#перебором в цикле некоторого диапазона значений лямбда и расчёта линейной корреляции 
#(stats::cor).
#Пример реализации:
#подбор значений степени(лямбда) в функции
find_lambda <- function(x, y, start = -5, end = 5, by = 0.1) {
  # Диапазон значений лямбда
  l <- seq(start, end, by)
  # Трансформация
  d <- outer(x, l, "^")
  # Изменяем знак
  d[, l < 0] <- -d[, l < 0]
  # Для случая, где лямбда равна нулю
  d[, l == 0] <- log(x)
  # Расчёт корреляций
  r <- cor(d, y)[, 1]
  l[which.max(abs(r))]
}

x <- mtcars$hp
y <- mtcars$mpg

find_lambda(x, y)

#Выяснили, что при Лямбда = -0.7 взаимосвязь максимально линейна
fit1 <- lm(mpg~hp, mtcars)
fit2 <- lm(mpg~I(-hp^-0.7), mtcars)

summary(fit1)
summary(fit2)#коэффициент b1 получился равный -444, что сложно для интерпретации взаимосвязи
#
##логорифмирование помогает решить проблему нечитаемости степенного преобразования
#
qplot(x = log(hp), y = log(mpg), data = mtcars) #позволяет бороться с нелинейностью
fit3 <- lm(log(mpg) ~ log(hp), mtcars) 
summary(fit3)                            #log(hp)   b1 =  -0.53009 
#
#От логорифмов можно перейти к интерпретации исходных данных
#
#2)Трансформация Тьюки(логорифмирование):
#
#y1 = b0+b1*x1         x2>x1
#y2 = b0+b1*x2
#
#y2-y1 = b1*(x2-x1) если допустить, что x2-x1 = 1(единичное изменение), тогда y2-y1 = 1
#
#ситуация с логорифмами
#log(y1) = b0+b1*log(x1)
#log(y2) = b0+b1*log(x2)
#
#log(y2)-log(y1) = b1*(log(x2)-log(x1))
#
#log(y2/y1) = b1*log(x2/x1)
#
#log(y2/y1) = log((x2/x1)^b1) => y2/y1 = (x2/x1)^b1
#
#log(y) = b0+b1*log(x), допустим b1 = 3, а x[0;200]
#возьмем x1 = 100, x2 = 101, 
#тогда
#y2/y1 = (101/100)^3 = 1.03, то есть y увеличится на 3%(=b1) при единичном изменении x
#
#то есть, коэффициент b1 в уравнении с логорифмами означает количество процентов увеличения 
#y(зависимой переменной) при единичном увеличении x
summary(fit3) 
#в нашем случае b1 = -0.53, то есть yменьшается на -0.53% при единичной взаимосвязи
#
#можно применять методы логорифмирования и возведения в степень к разным частям уравнения
#к зависимой переменной, ко всем предикторам, или к некоторым предикторам
#В видео я упомянул, что логарифмическая трансформация только зависимой переменной 
#или только независимой переменной также оставляет возможность удобной интерпретации:
#
#В модели log(Y)=b1???X+b0 коэффициент наклона означает: при единичном изменении 
#переменной X, переменная Y в среднем изменяется на 100???b1 процентов.

#В модели Y=b1???log(X)+b0 коэффициент наклона означает:  изменение на 1% по X в 
#среднем приводит к 0.01???b1 изменению по переменной Y.
#
#Строго говоря, в модели log(Y)=b1???X+b0 при увеличении X на единицу переменная Y 
#в среднем изменится на 100(eb1???1) процентов. Выражение 100???b1 можно рассматривать 
#лишь как линейное приближение точного выражения при значениях b1, близких к нулю.
#
#Точно так же в модели log(Y)=b1???log(X)+b0 при увеличении X на 1% переменная 
#Y в действительности изменится на 100((101/100)b1???1) процентов, что мало отличается от 
#b1 только при b1, близких к нулю. Например, уже при b1=100 приращение Y составит не 100%, 
#а около 170%. С увеличением b1 по модулю расхождение между указанными выражениями будет 
#возрастать неограниченно. 
#
#Нелинейность взаимосвази очень часто сопровождается ненормальностью распределения остатков
hist(fit1$residuals) #гистограмма распределения остатков
shapiro.test(fit1$residuals) #p-value = 0.02568, есть основание отклонить H0 о нормальности
shapiro.test(fit2$residuals) #p-value = 0.1077, остатки распределены нормально
shapiro.test(fit3$residuals) #p-value = 0.3982, для логорифмирования самый лучший вариант
#
#Когда наши данные взаимосвязаны нелинейно, можно посмотреть на распределение остатков,
#при любом количестве предикторов, и явно увидеть нелинейность взаимосвязи через
#ненормальность распределения остатков
#Трансформация Бокса — Кокса (Box-Cox transformation) — широко используемый метод 
#трансформации данных. В контексте регрессии он обычно используется для трансформации 
#зависимой переменной в случае, если у нас есть ненормальное распределение ошибок и/или 
#нелинейность взаимосвязи, а также в случае гетероскедастичности.
#
#Идея трансформации очень простая:
#ynew=yp???1p  , если p???0 
#ynew=log(y) , если p=0  
#
#Параметр p подбирается по схожей идее: мы будем использовать то p, при котором качество 
#модели максимально (обычно используется метод максимального правдоподобия). 
#Например, в случае множественной регрессии мы можем трансформировать зависимую 
#переменную, чтобы добиться более высокого качества модели и выполнения требования к данным.
#https://r-analytics.blogspot.com/2015/07/blog-post_19.html
#
#
###Проблема гетероскедастичности(неравномерность распределения остатков)
library(ggplot2)
library(dplyr)
diamonds_2 <- sample_n(diamonds, 500)
qplot(x = price, y = carat, data = diamonds_2)+
  geom_smooth(method=lm)                          #амплитуда ошибки возрастает 
fit_1 <- lm(carat~price, diamonds_2) 
coefficients(fit_1)   #коэффициенты не отражают зависимость, предсказания будут неточны
plot(fit_1)              #хороший график для оценки распределения
#
#Если мы используем ошибки(остатки) в качестве зависимой переменной, а в качестве 
#независимой - предикторы, и получим высокий R^2 - это означает что изменение ошибок
#(остатков) зависит от изменения предиктора, что свидетельствует о гетероскедастичности
#
#Основной тест для поиска гетероскедастичности - Тест Уайта(и еще Тест Бройша — Пагана)
#в качестве зависимой переменной берутся ошибки, в качестве независимой - предикторы
#
#Как избавиться от гетероскедастичности:
#Трансформации помогают избавиться от гетероскедастичности: Box-Cox или степенная
#Box-Cox трансформация решает сразу несколько проблем: нелинейность взаимосвязи и
#гетероскедастичность
#
qplot(x = log(price), y = log(carat), data = diamonds_2)+
  geom_smooth(method = lm)
library(lmtest)
bptest(lm(price~carat, diamonds_2)) #тест Бройша-Пагана показывает гетероскедастичность
bptest(lm(log(price)~log(carat), diamonds_2)) #результат лучше
bptest(lm(log(carat)~log(price), diamonds_2)) 
fit_2 <- lm(log(price)~log(carat), diamonds_2)
shapiro.test(fit_2$residuals) #нормальность распределения остатков
#
##Проблема мультиколлинеарности
#Наличие линейной зависимости между предикторами регрессионной модели
#
#Мультиколлинеарность бывает двух типов:
#1)Полная мультиколлинеарность
#2)Частичная мультиколлинеарность
set.seed(42)
d <- data_frame(y=rnorm(30),
                x_1=rnorm(30),
                x_2=x_1,
                x_3=rnorm(30),)

pairs(d)      #графика scatter_plot для визуализации связи переменных
fit <- lm(y~., d) #если напротив какой-то переменной индекс NA -значит она связана с другой
summary(fit)      
#
#Несмотря на то, что регрессия учитывает корреляцию между предикторами,
#наличие сильной корреляции между предикторами, может иногда давать неожиданные результаты
#Пример:

library(ggplot2)
qplot(x = speed, y = dist, data = cars)
fit_1 <- lm(dist~speed, cars)
summary(fit_1)

cars <- mutate(cars, speed_2 = speed^2, speed_3 = speed^3) #новые предикторы

fit_2 <- lm(dist~., cars) #от трех предикторов
summary(fit_2) #коэффициенты стали статистически незначимыми
#
#Вывод: Мультиколлинеарность может негативно влиять на p-уровень значимости
#
#Вспоминаем формулу дисперсии коэффициентов
#Допустим y = b0 +b1*x
#
#Мы хотим проверить H0 о том, что x значимо связана с y
#Тогда введем гипотезу Н0: b1 == 0
#
#Если верна нулевая гипотеза, то повторяя многократно наш эксперимент и рассчитывая:
# t= (bj-Bj)/sebj, по которому мы модем посчитать p-уровень значимости
#
#var(bj) = s^2/((n-1)*var(xj)*(1-Rj^2)) #где var(bj) - дисперсия b1, var(xj) - дисперсия xj,
#s^2 - дисперсия остатков
#
#Дисперсия коэффициента b1 при Х тем больше чем:
#1)Больше дисперсия остатков
#2)Меньше дисперсия самого Х
#3)Больше R^2 регрессионной модели: x~другие предикторы
#
#Таким образом если в нашей модели есть сильная корреляция между предикторами, но 
#пострадает стандартная ошибка (Sebj) каждого коэффициента, что сильно расширит 
#доверительные интервалы, и значимость коэффициентов снизится
#
#Мультиколлинеарность может подтолкнуть нас к ложным выводам.
#Пример на основе данных swiss

fit1 <-lm(Fertility~., swiss) 
summary(fit1)
#
#КОээфициент при Examination оказался статистически не значим
#Но причина почему это так не ясна.
#Рассчитаем корреляцию между Fertility и Examination
cor.test(~ Fertility + Examination, swiss)
#
#выясняется, что корреляция отрицательная, и статистически значима
#
#То есть необходимо сначала убедиться в том, что в наших данных нет мультиколлинеарности
#прежде чем делать вывод о статистической значимости каких-либо коэффициентов в регрессии
#
#Существует несколько способов проверки на мультиколлинеарность:
#1)Построить корреляционную матрицу и проверить корреляцию между всеми предикторами
#2)Для каждого предиктора посмотреть насколько хорошо он объясняется другими предикторами:
#Часть формулы var(bj) = s^2/((n-1)*var(xj)*(1-Rj^2))
#VIF = (1-Rj^2) - коэффициент Variance inflation factor(VIF) показывает насколько хорошо
#выбранный предиктор объясняется другими предикторами
#
#Для каждого коэффициента в y=b0+b1*x1+b2*x2+b3*x3
#мы можем посчитать свой VIF коэффициент.
#Например x1~x2+x3, и если R^2 такой модели будет высок, это покажет что между предикторами
#имеется сильная зависимость
#
#Интерпретация VIF:
#sqrt(VIF) показывает во сколько раз больше стала стандартная ошибка данного коэффициента
#по сравнению с ситуацией, если бы данный предиктор был бы абсолютно независим от других
#Пример:
#Если при некотором предикторе VIF равен 5.27(sqrt(VIF) = 2.3) это означает, что стандартная
#ошибка этого коэффициента в 2.3 раза больше, по сравнению с ситуацией когда предиктор был 
#бы нескоррелирован с другими.
#Примеры расчета:

library(dplyr)
library(car)
library(DAAG)
fit1 <-lm(Fertility~., swiss) 
summary(fit1)
cor.test(~ Fertility + Examination, swiss) #корреляция есть, а в регрессии нет

vif(fit1)         #расчет VIF для fit1

#VIF(Examination) имеет самое большое значение, это переменную можно исключить
fit2 <- lm(Fertility~., select(swiss, -Examination)) #исключаем Examination
summary(fit2) 
#
#R^2 не изменился
#
#Считается, что если VIF>10, то нужно исключить предиктор с таким VIF
#
#Если задача в предсказании, то VIF не так страшен, но если проверять каждый коэффициент
#отдельно, то надо проверять наши данные на мультиколлинеарность, потому что можно ошибочно
#решить, что какие-то предикторы не связаны с зависимой переменной
#
#Задача 1
hetero_test <-  function(test_data){
  fit <- lm(test_data[,1] ~ ., test_data[-1])          #можно вставить вектор
  fit_2 <- lm(fit$residuals^2 ~.,test_data[-1])
  summary(fit_2)$r.squared 
}

hetero_test <-  function(test_data){
  formula1 <- as.formula(
    paste(
      names(test_data)[1],'~',paste(names(test_data)[-1], collapse ='+')
    )
  )
  fit1 <- lm(formula1, test_data)
  test_data$residuals <- fit1$residuals^2
  formula2 <- as.formula(
    paste(
      "residuals",'~',paste(names(test_data)[-c(1,ncol(test_data))], collapse ='+')
    )
  )
  fit2 <- lm(formula2, test_data)
  unlist(summary(fit2))$r.squared
}

hetero_test(mtcars)
test_data <- mtcars

formula <- as.formula(
  paste(
    names(test_data)[1],'~',paste(names(test_data)[-1], collapse ='+')
    )
  )

fit$residuals
reformulate(termlabels = listoffactors, response = 'y') #как вариант универсальной функции
#конец задачи 1

#Задача 2
VIF <-  function(test_data){
  data_1 <- test_data[,-1]
  r_sq <- sapply(c(1:length(data_1)), function(x)summary(lm(data_1[, x]~.,data_1[-x]))$r.squared)
  VIF_X <- sapply(r_sq, function(x) 1/(1-x))
  names(VIF_X) <- names(data_1)
  return(VIF_X)
}

VIF <- function(d) {
  x <- d[,-1]
  sapply(c(1:length(x)), function(y) 1/(1-summary(lm(x[,y]~., x[-y]))$r.sq))
}


VIF <- function(d) {
  x <- d[,-1]
  sapply(names(x), function(y) 1/(1-summary(lm(as.formula(paste(y,"~.")), x))$r.sq) )
}


d <- mtcars
sapply(names(x), function(y) summary(lm(as.formula(paste(y,"~.")), x))$r.sq )

VIF <- function(data)
{
  sapply(names(data[-1]), function(name) {
    m = lm(as.formula(paste(name, "~ .")), data[-1])
    r2 = summary(m)$r.squared
    1 / (1 - r2)})
}

VIF(mtcars)

VIF(test_data)
test_data <- mtcars
test_data <- data.frame(y = rnorm(30, 5), x1 = rnorm(30, 5))
test_data$x2 <- test_data$x1^2

apply(data, 2, function(x) summary(lm(x~., data[-x]))$r.squared)
s <- sapply(names(data), function(x) select(data, -x), simplify = F)

#Конец задачи 2

#Задача 3
#мой вариант
VIF <- function(d) {
  x <- d[,-1]
  sapply(names(x), function(y) 1/(1-summary(lm(as.formula(paste(y,"~.")), x))$r.sq) )
}
smart_model <-  function(test_data){
  while(any(VIF(test_data)>10)){
    test_data <- test_data[-(which.max(VIF(test_data))+1)]
  }
  return(lm(test_data)$coefficients)
}

#Вариант с рекурсией

VIF <- function(d) {
  x <- d[,-1]
  if (ncol(d) > 2) {
    sapply(names(x), function(y) 1/(1-summary(lm(as.formula(paste(y,"~.")), x))$r.sq) )
  } else 0
}

smart_model <- function(d) {
  vif <- VIF(d)
  if (max(vif) > 10) { 
    smart_model(d[, -(which.max(vif) + 1)])
  } else lm(d)$coeff
}

#выражения lm(data)  ===  lm(data[, 1] ~ ., data[-1]) эквивалентны

smart_model(test_data)

sapply(c(1:length(test_data)), function(x)VIF(test_data), simplify = F)

sapply(test_data, function(x)VIF(test_data), simplify = F)

sapply(test_data, function(x)test_data[-(which.max(VIF(test_data))+1)], )

#конец задачи 3

#Задача 4
#мой вариант
transform_x <-  function(test_data){
  s1 <- c(seq(-2,-0.1,0.1),seq(0.1,2,0.1))
  s <- sapply(c(seq(-2,-0.1,0.1),seq(0.1,2,0.1)), function(z)cor(test_data$x^z,test_data$y))
  s[41] <- cor(log(test_data$x),test_data$y)
  if(which.max(abs(s))==41){return(log(test_data$x))}
  if(which.max(abs(s))>20){return(test_data$x^s1[which.max(abs(s))])}
  if(which.max(abs(s))<=20){return(-test_data$x^s1[which.max(abs(s))])}
}
#второй вариант
transform_x <- function(test_data) {
  y <- test_data[[1]]
  x <- test_data[[2]]
  # Диапазон значений лямбда
  l <- seq(-2, 2, 0.1)
  # Трансформация
  d <- outer(x, l, "^")
  # Изменяем знак
  d[, l < 0] <- -d[, l < 0]
  # Для случая, где лямбда равна нулю
  d[, l == 0] <- log(x)
  # Расчёт корреляций
  r <- cor(d, y)[, 1]
  # Выбор оптимального значения
  d[, which.max(abs(r))]
}
#третий вариант
transform_x = function(data)
{
  do_transform = function(x, lambd) {
    if (lambd > 0) x ^ lambd else if (lambd < 0) -(x ^ lambd) else log(x) }
  
  x_data = data[,2] 
  y_data = data[,1]
  lambdas = seq(-2, 2, 0.1)
  corrs = sapply(lambdas, 
                 function(lambd) cor(do_transform(x_data, lambd), y_data))
  lambda = lambdas[which.max(abs(corrs))]
  #print(lambda)
  do_transform(x_data, lambda)
}

test_data <- as.data.frame(list(y = c(0.05, 0.06, 0.06, 0.06, 0.07, 0.06, 0.07, 0.07, 0.05, 0.05, 0.08, 0.06, 0.06, 0.06, 0.12, 0.06, 0.05, 0.05, 0.06, 0.06), x = c(11.69, 10.11, 9.86, 10.61, 8.79, 10.15, 8.78, 8.28, 12.38, 12.24, 8.65, 10.15, 11.17, 9.7, 5.97, 11.02, 11.56, 12.43, 10.64, 10.86)))
test_data <- as.data.frame(list(y = c(1.27, 1.2, 1.25, 1.17, 1.27, 1.25, 1.31, 1.23, 1.24, 1.27, 1.24, 1.22, 1.26, 1.28, 1.24, 1.27, 1.25, 1.24, 1.25, 1.27), x = c(11.61, 6.73, 9.59, 5.29, 11.55, 9.13, 14.42, 7.73, 8.44, 10.75, 8.37, 7.12, 10.6, 12.13, 8.52, 10.89, 9.33, 8.67, 9.64, 11.24)))
transform_x(test_data)

set.seed(42)
test_data <- data.frame(y = rnorm(10, 10, 1), x = rnorm(10, 10, 1)) 
c(seq(-2,-0.1,0.1),seq(0.1,2,0.1))

cor(test_data$x,test_data$y)

#конец задачи 4
