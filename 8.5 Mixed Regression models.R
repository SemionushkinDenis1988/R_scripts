########Смешанные регрессионные модели
#
#Что делать если не соблюдается требование к независимости наблюдений?
#Если наблюдения зависимы, на помощь приходят Смешанные регрессионные модели.
#
#Мудрость толпы - условное понятие, когда большое количество наблюдений, в силу своего
#количества дает понятие, близкое к реальному. Например если попросить 100000 человек
#оценить вес быка, то медианное значение суммы их ответов будет очень близко к реальному
#возможно даже блже чем случайная ошибка.
#
#Как работает мудрость толпу?
#
#Если спросить 100000 людей о весе быка, то на результат опроса будут влиять многие
#СЛУЧАЙНЫЕ фактор, которые будут распределены равномерно и поэтому отрицательные
#факторы скорее всего будут компенсированы положительными факторами.
#
#А если факторы влияющие на результат опроса будут не случайными, а систематическими, 
#например все опрошенные будут уставшие, тогда это сильно сместит результаты опроса 
#относительно истиного значения
#
#Принцип мудрости толпы лежит в основе статистических методов, в том, допущении, что ошибки
#случайны и не связаны друг с другом
#
#Но зачастую в проверяемых наблюдениях сложно соблюсти на 100% фактор независимости
#между наблюдениями, поэтому применяется ряд мер по борьбе с этим явлением
#
#Источники зависимости:
#1)Повторные измерения на одном и том объекте(например два замера на одном больном)
#2)Повторные пробы на одном и том же уровне независимой переменной
#3)Кластеризация данных. Нет повторных измерений, но данные взяты из гомогенных групп
#Например тестирование в нескольких больницах(каждая больница это кластер)
#
#Проблемы, которые возникают из-за вышеуказанных факторов:
#1)Повторные измерения: 
#а)снижение чувствительности теста:
##если мы проводим 100 измерений, по 20 на 5 людях, данных много, но данные сгруппированны
#по 5 испытаемым, что приводит к группировке, если строить интервал по 100 независимым
#наблюдениям доверительный интервал будет маленький, а если по 5, то большой - это приведет
#к ошибке при принятии или отклонении H0
#б)Искуственное увеличение мощности теста(псевдорепликация):
#если мы проводим 100 измерений, по 20 на 5 людях, данных много, но данные сгруппированны
#по 5 испытаемым, что приводит к группировке, если строить интервал по 100 независимым
#наблюдениям доверительный интервал будет маленький, а если по 5, то большой - это приведет
#к ошибке при принятии или отклонении H0
#2)Несбалансированный дизайн: 
#a)Искажение результатов. Например, когда одних испытаний больше чем других, например
#если мы дали больному плацебо 50 раз а настоящее лекарство 100 раз
#3)Кластеризация данных:
#a)Искажение результатов. Например, когда одних испытаний больше чем других, например
#если мы дали больному плацебо 50 раз а настоящее лекарство 100 раз
#
#Основные понятия:
#Эффект - влияние независимой переменной на зависимую
#Типы эффектов:
#a)Фиксированный эффект(главный эффект) - эффекты, которые мы ожидаем, такие эффекты  
#представляют основной интерес для исследователя
#б)Случайный эффект - такие эффекты не представляют первостепенного интереса для 
#исследователя, например разница между больницами, в которых мы брали данные
#
#Например:
#Случайные эффекты(то что мы не хотим варьировать, но оно присутствует):
#а)Испытуемый
#б)Пол
#в)Возраст
#г)Стимул
#д)Город
#е)Социальная сеть
#
#Главные эффекты(то что мы хотим варьировать):
#а)Независимые переменные, которые мы исследуем(выздоровел/не выздоровел - это эффект)
#
#Линейная регрессия с фиксированными эффектами
#
#y = xb(0,1) +zl + e, где zl - случайный эффект, а e - необъяснимая дисперсия
#
install.packages('lme4')
install.packages('mlmRev')
install.packages('lmerTest')
# lmer - аналог lm для смешанной регрессии(linear mixed effects regression)

lmer(DV ~ FE + (1 + FE | RE), data = my_data)#FE - независимая переменная fixed effect
#RE - random effect, 1 - свободный член

install.packages('lme4')
install.packages('mlmRev') 
install.packages('lmerTest')

library(mlmRev)
library(lme4)

data("Exam")

str(Exam)
help(Exam)

library(ggplot2)

#мы будем изучать, как связана вступительная оценка с выпускной, то есть влияние 
#StandLRT на normexam

ggplot(data = Exam, aes(x = standLRT, y = normexam)) + 
  geom_point()


ggplot(data = Exam, aes(x = standLRT, y = normexam, col = school)) + 
  geom_point()

#Сначала создадим простую линейную регрессию с одним главным эффектом

Model1 <- lm(normexam ~ standLRT, data=Exam)
summary(Model1)

#строим на графике регрессионную прямуж предсказывающую значения normexam

Exam$Model1_pred <- predict(Model1)
ggplot(data = Exam, aes(x = standLRT, y = normexam)) + 
  geom_point() + 
  geom_line(data = Exam, aes(x = standLRT, y = Model1_pred), col = 'blue', size = 1)

#Модель со случайным эффектом
Model2 <- lmer(normexam ~ standLRT + (1|school), data=Exam) #случайный эффект для школы
#в саммари мы можем видеть коэффициенты для Random и Fixed эффектов
summary(Model2)

#Наносим на диаграмму рассеивания нашу модель со случайным эффектом
#теперь для каждоый школы построена своя регрессионная прямая, все прямые параллельны,
#отличается лишь итерцепт
Exam$Model2_pred <- predict(Model2)
ggplot(data = Exam, aes(x = standLRT, y = normexam)) + 
  geom_point(alpha = 0.2) + 
  geom_line(data = Exam, aes(x = standLRT, y = Model2_pred, col = school))


#Модель с главным эффектом + случайный свободный член + случайный угловой коэффициент
#в этой модели мы считаем, что есть корреляция между случайными эффектами(интерцепт и слоп) 
Model3 <- lmer(normexam ~ standLRT + (1 + standLRT|school), data=Exam)
summary(Model3)

#теперь регрессионные прямые отличаются так же наклоном
Exam$Model3_pred <- predict(Model3)
ggplot(data = Exam, aes(x = standLRT, y = normexam)) + 
  geom_point(alpha = 0.2) + 
  geom_line(data = Exam, aes(x = standLRT, y = Model3_pred, col = school))


#Модель с главным эффектом и случайным угловой коэффициентом
Model4 <- lmer(normexam ~ standLRT + (0 + standLRT|school), data=Exam)
summary(Model4)

Exam$Model4_pred <- predict(Model4)
ggplot(data = Exam, aes(x = standLRT, y = normexam)) + 
  geom_point(alpha = 0.2) + 
  geom_line(data = Exam, aes(x = standLRT, y = Model4_pred, col = school))

#МОдель для нескоррелированных случайных эффектов - случайный свободный член надо записать
#отдельно от случайного углового коэффициента
Model5 <- lmer(normexam ~ standLRT + (1|school) + (0 + standLRT|school), data=Exam)
summary(Model5)


#Применение смешанных моделей позволяет решить проблему псевдорепликации и зависимости 
#наблюдений с помощью включения случайных эффектов в модель.
#Для косвенной оценки статистической значимости предиктора можно использовать t-значение.
#Для случайных эффектов возможно рассчитать и intercept, и коэффициент наклона.

#Реализация случайных эффектов в обычных линейных регрессиях
#1)lm(standLRT ~ normexam + school, data = Exam)
#Модель, предсказывающая standLRT по normexam с учётом того, что все школы различаются по 
#среднему уровню сдачи выпускного экзамена (один случайный эффект: свободный член по 
#переменной “school”)

#2)lm(standLRT ~ normexam + school:normexam, data = Exam)
#Модель, предсказывающая standLRT по normexam с учётом того, что разные школы отличаются по 
#взаимосвязи между предиктором и зависимой переменной (один случайный эффект: угловой 
#коэффициент по переменной “school”)

#3)lm(standLRT ~ normexam*school, data = Exam)
#Модель, предсказывающая standLRT по normexam с учётом того, что разные школы отличаются 
#по среднему уровню сдачи выпускного экзамена, а также по взаимосвязи между предиктором и 
#зависимой переменной (два случайных эффекта: свободный член и угловой коэффициент по 
#переменной “school”)

###################################################################################

#Статистическая значимость, обобщённые модели и случайные эффекты

#для принятия решения о важности или не важности фактора мы сравниваем модели между собой
#чтобы провести сравнение моделей надо добавить аргумент REML = FALSE, то есть мы 
#используем не метод REML
Model2 <- lmer(normexam ~ standLRT + (1|school), REML = FALSE, data=Exam)
summary(Model2) #  AIC      BIC   logLik deviance df.resid  позволяют оценить

#для сравнения создаем "пустую модель
Model0 <- lmer(normexam ~ 1 + (1|school), REML = FALSE, data = Exam)
summary(Model0)

anova(Model0, Model2) #сравнить две модели мы можем с помощью F-test(дисперсионный анализ)
#модель 1 статистически значимо отличается от модели 0 (p = 2.2e-16)

#для поправку к степеням свободы используется поправка Satterthwaite's method
#для её использования нужен пакет lmerTest(апроксимация количества степеней свободы)
library(lmerTest)

Model2 <- lmer(normexam ~ standLRT + (1|school), data=Exam)
summary(Model2)

#Обычная линейная регрессия является членом семейства в котором зависимая переменная 
#распределена нормально, в других регрессиях распределение иное, например в 
#логистической регрессии распределение биноминальное

#реализация логистической регрессии с учетом смешанных эффектов
Exam$school_type <- ifelse(Exam$type == 'Mxd', 1, 0)

Model5 <- glmer(school_type ~ normexam + (1|school), family = "binomial", data = Exam)

summary(Model5) #сразу получаем p-значение по z-value

#Предсказание на текущем датасете(можно его не указывать, он указан по-умолчанию)
predict(Model2, Exam)

#чтобы предсказать на новом дата необходимо установить его в функцию предикт
new_Exam <- Exam[sample(1:nrow(Exam), 100), ]
new_Exam$school <- sample(101:200)
#чтобы предсказать на новом дата необходимо установить его в функцию предикт
predict(Model2, new_Exam, allow.new.levels = T) #аргумент allow.new.levels = T принимает
#новые уровни случайного эффекта

#функция, которая позволяет получить информацию о фиксированных эффектах
fixef(Model3) #возвращает коэффициенты для фиксированного эффектов
#функция, которая позволяет получить информацию о случайных(random) эффектах
ranef(Model3) #возвращает коэффициенты для случайных эффектов

#Задача 1 #проверяем различия по attitude
library(ggplot2) 
plot_1 <- ggplot(exp_data, aes(x = factor(scenario), y = frequency, fill = attitude))+
  geom_boxplot()
            

exp_data <- read.csv("politeness_data.csv")
str(exp_data)

#Конец задачи 1
#Задача 2            #проверяем различия по gender
plot_2 <- ggplot(exp_data, aes(x = frequency, fill = subject))+
  geom_density(alpha = 0.5)+
  facet_grid(gender~.)

#Конец задачи 2

#Задача 3        #случайный эффект - интерцепт subject и scenario
library(lme4)
fit_1 <- lmer(frequency ~ attitude + (1|subject) + (1|scenario), data=exp_data)
#конец задачи 3

#Задача 4 #добавляем фиксированный эффект gender

fit_2 <- lmer(frequency ~ attitude + gender + (1|subject) + (1|scenario), data=exp_data)

#Задача 5 добавляем случайные эффекты наклона(slope)

fit_3 <- lmer(
  frequency ~ attitude + gender + (1+ attitude|subject) + (1+ attitude|scenario), 
  data=exp_data)

